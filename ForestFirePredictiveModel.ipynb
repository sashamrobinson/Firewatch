{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e30724bc-ae76-4530-ad77-b45259f6814c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9852829a-b4d3-4c32-8b3b-10a569aef6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Citation:\n",
    "# P. Cortez and A. Morais. A Data Mining Approach to Predict Forest Fires using Meteorological Data.\n",
    "# In J. Neves, M. F. Santos and J. Machado Eds., New Trends in Artificial Intelligence,\n",
    "# Proceedings of the 13th EPIA 2007 - Portuguese Conference on Artificial Intelligence, December,\n",
    "Guimaraes, Portugal, pp. 512-523, 2007. APPIA, ISBN-13 978-989-95618-0-9.\n",
    "\"\"\"\n",
    "df = pd.read_csv(\"forestfires.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f053136-4629-4151-aa15-9d7da5fe90bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data for model by translating to booleans\n",
    "feats = ['month','day']\n",
    "data = pd.get_dummies(df,columns=feats,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9a9321a8-109a-4a65-b45c-29546c69b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data for training\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Remove area field (this is the expected evaluative output)\n",
    "X = data.drop(['area'],axis=1).values\n",
    "y = data['area'].values\n",
    "\n",
    "# 80% of data for training, 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "722a4444-c7f1-4a4c-8f4a-4e1b299bc099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling data to fit model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c03c89b6-aff8-4297-85d5-478f526eaecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning with keras using TensorFlow\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\"\"\"\n",
    "Hidden layers are calculated as:\n",
    "        (input_node_count + output_node_count) / 2 = 7\n",
    "\"\"\"\n",
    "\n",
    "# Input layer\n",
    "model.add(Dense(7, kernel_initializer = \"uniform\",activation = \"relu\", input_dim=27))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, kernel_initializer = \"uniform\",activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "18673692-a60e-4a3a-b600-699c7c15f9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 419us/step - loss: 0.4850 - accuracy: 0.0604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x294117310>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gradient descent\n",
    "model.compile(optimizer= \"adam\",loss = \"binary_crossentropy\",metrics = [\"accuracy\"])\n",
    "\n",
    "# Fitting model to data\n",
    "model.fit(X_train, y_train, batch_size = 10, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e56e16eb-b6a0-4116-948b-fa17bd48b6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 833us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "411b65d5-0ae7-466b-8c76-929e39b3c4f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_absolute_error\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     27\u001b[0m loss, mae \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess your dataset\n",
    "# X: Features, y: Target variable (amount of area burned)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a sequential neural network model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(X_train.shape[1],)),  # Input layer\n",
    "    keras.layers.Dense(128, activation='relu'),  # Hidden layer with ReLU activation\n",
    "    keras.layers.Dense(64, activation='relu'),   # Another hidden layer\n",
    "    keras.layers.Dense(1)  # Output layer (1 neuron for regression)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_absolute_error'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Mean Absolute Error on Test Data: {mae}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39cef0e-133e-4b56-a507-6c6bab56cf47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
